\section{State-of-the-art Techniques}

Iterative methods usually implement a combination of three ingredients:

\begin{enumerate}
\item \textbf{Step Computation} defines how steps are computed. Examples include solutions
of linear programs (LPs) or quadratic programs (QPs).
\item \textbf{Globalization Strategy} defines whether a step is acceptable, and makes progress towards
a solution. Examples include sufficient reduction for an exact penalty function, or
acceptance for a filter.
\item \textbf{Globalization Mechanism} defines the recourse action that an algorithm takes when a step
is not acceptable. Examples include a line-search or a trust-region.
\end{enumerate}

They share a common framework (Algorithm \ref{alg:iterative-methods}).

\medskip
\begin{algorithm}[H]
\medskip
\caption{Common framework for iterative methods}
\label{alg:iterative-methods}
Given initial primal-dual estimate $(x^{(0)}, \lambda^{(0)}, \nu^{(0)})$ \;
Set $k \gets 0$ \;
\While{termination criteria are not met} {
	Compute acceptable step $(s^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)})$ by solving approximately \eqref{eq:NLP} at $(x^{(k)}, \lambda^{(k)})$ \;
	Compute new primal iterate $\hat{x}^{(k)} := x^{(k)} + s^{(k)}$ \;
	Update $(x^{(k+1)}, \lambda^{(k+1)}, \nu^{(k+1)}) := (\hat{x}^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)})$ \;
	Set $k \gets k+1$ \;
}
\end{algorithm}
\medskip

\subsection{Local Approximations}

\subsubsection{Quadratic Subproblem}

Solve a QP subproblem approximation of \eqref{eq:NLP} at the point $(x^{(k)}, \lambda^{(k)})$:

\begin{equation}
\label{eq:qp-subproblem}
\begin{array}{lll} \dps
\mini_d & \nabla f(x^{(k)})^T d + \frac{1}{2} d^T W_1(x^{(k)}, \lambda^{(k)}) d \\
\st 	& \underline{c} \le c(x^{(k)}) + \nabla c(x^{(k)})^T d \le \overline{c} \\
		& \underline{x} \le x^{(k)} + d \le \overline{x} \\
%                & \lVert d \rVert_\infty \le \rho_k, 
\end{array}
\end{equation}

Second-order convergence near a solution

Talk about Maratos effect

\subsection{Globalization Strategies}

Strategies handle infeasibility

\subsubsection{Penalty Function Methods}
Relaxed NLP with penalty parameter $\sigma$

\begin{equation}
\begin{array}{ll} \dps
\mini_x & \sigma f(x) + \sum\limits_{j = 1}^m \max(0, \underline{c_j} - c_j(x), c_j(x) - \overline{c_j}) \\
\st 	& \underline{x} \le x \le \overline{x} \\
\end{array}
\end{equation}

Smooth version

\begin{equation}
\begin{array}{ll} \dps
\mini_{x,s} & \sigma f(x) + \sum\limits_{j = 1}^m s_j \\
\st 		& \underline{c} - c(x) \le s \\
			& c(x) - \overline{c} \le s \\
			& \underline{x} \le x \le \overline{x} \\
			& 0 \le s \\
\end{array}
\end{equation}

\charlie{cite Infeasibility detection paper and describe method}

\subsubsection{Modern Penalty Function Methods (Nocedal)}

\subsubsection{Classical Penalty Function Methods (Practical Methods of Optimization)}

\subsubsection{Filter}

Constraint violation for optimality phase
\begin{equation}
h(x) = \sum\limits_{j=1}^m \bigg\lVert \max(0, \underline{c_j} - c_j(x), c_j(x) - \overline{c_j}) \bigg\rVert_1
\end{equation}

If the subproblem (e.g \ref{eq:qp-subproblem}) is infeasible, our filter method takes this as an indication that
\eqref{eq:NLP} may be infeasible, and solves a feasibility restoration subproblem to get closer to the
feasible region. This feasibility restoration problem is defined as:

\begin{equation}
\label{eq:restoration-subproblem}
\begin{array}{llll}
\underset{d}{\text{minimize}} 	& \multicolumn{3}{l}{\sum\limits_{j \in \overline{\mathcal{V}}^{(k)}} \nabla c_j(x^{(k)})^T d - \sum\limits_{j \in \underline{\mathcal{V}}^{(k)}} \nabla c_j(x^{(k)})^T d + \frac{1}{2} d^T W_0(x^{(k)}, \hat{\lambda}^{(k)}) d} \\
\text{subject to} 		& \underline{c_j} \le c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d \le \overline{c_j}, & & \forall j \in \mathcal{S}^{(k)} \\
				& \overline{c_j} \le c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d, & & \forall j \in \overline{\mathcal{V}}^{(k)} \\
				& c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d \le \underline{c_j}, & & \forall j \in \underline{\mathcal{V}}^{(k)} \\
				& \underline{x_i} \le x_i^{(k)} + d_i \le \overline{x_i}, & & \forall i \in \{1, \ldots, n\} \\
%				& \lVert d \rVert_\infty \le \rho^{(k)}
\end{array}
\end{equation}

where the sets $\mathcal{S}$, $\overline{\mathcal{V}}$ and $\underline{\mathcal{V}}$ are a partition of the linearized constraints:
\begin{align}
\mathcal{S}^{(k)} 				& = \{j ~|~ \underline{c_j} \le c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d \le \overline{c_j} \} \\
\overline{\mathcal{V}}^{(k)} 	& = \{j ~|~ \overline{c_j} < c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d \} \\
\underline{\mathcal{V}}^{(k)} 	& = \{j ~|~ c_j(x^{(k)}) + \nabla c_j(x^{(k)})^T d < \underline{c_j} \}
\end{align}

and the multipliers $\hat{\lambda}^{(k)}$ are defined as:

\begin{equation}
\hat{\lambda}_j^{(k)} =
\begin{cases}
\lambda_j^{(k)} & \text{if } j \in \mathcal{S}(x^{(k)}) \\
1				& \text{if } j \in \underline{\mathcal{V}}^{(k)} \\
-1				& \text{if } j \in \overline{\mathcal{V}}^{(k)} \\
\end{cases}
\end{equation}

%where $\hat{W}^{(k)}$ is the Lagrangian Hessian at the point $(x^{(k)}, \lambda^{(k)})$:

%\begin{equation}
%\hat{W}^{(k)} = \sum_{j \in \overline{\mathcal{V}}^{(k)}} \nabla^2 c_j(x^{(k)}) -
%\sum_{j \in \underline{\mathcal{V}}^{(k)}} \nabla^2 c_j(x^{(k)}) -
%\sum_{j \in \mathcal{S}(x^{(k)})} \lambda_j^{(k)} \nabla^2 c_j(x^{(k)})
%\end{equation}

Constraint violation for feasibility restoration phase
\begin{equation}
h(x^{(k)}) = h_{\mathcal{S}^{(k)}} + h_{\mathcal{V}^{(k)}}
\end{equation}

where
\begin{align}
h_{\mathcal{S}^{(k)}} & = \sum\limits_{j \in \mathcal{S}^{(k)}} \bigg\lVert \max(0, \underline{c_j} - c_j^{(k)}, c_j^{(k)} - \overline{c_j}) \bigg\rVert_1 \\
h_{\mathcal{V}^{(k)}} & = h_{\overline{\mathcal{V}}^{(k)}} + h_{\underline{\mathcal{V}}^{(k)}} \\
h_{\overline{\mathcal{V}}^{(k)}} & = \sum\limits_{j \in \overline{\mathcal{V}}^{(k)}} \bigg\lVert \max(0, \underline{c_j} - c_j^{(k)}, c_j^{(k)} - \overline{c_j}) \bigg\rVert_1 \\
h_{\underline{\mathcal{V}}^{(k)}} & = \sum\limits_{j \in \underline{\mathcal{V}}^{(k)}} \bigg\lVert \max(0, \underline{c_j} - c_j^{(k)}, c_j^{(k)} - \overline{c_j}) \bigg\rVert_1
\end{align}

A filter $\mathcal{F}$ is a list of pairs of constraint violation and objective value $(h^{(l)}, f^{(l)})$, such that no
pair dominates another pair, that is there exists no indices $l, k \in \mathcal{F}$ such that

\begin{equation}
f^{(k)} < f^{(l)} \quad \text{and} \quad h^{(k)} < h^{(l)}
\end{equation}

Dominance alone is not enough to ensure convergence because filter iterates may accumulate near an infeasible filter
entry. To avoid this pitfall, we introduce a small margin around the filter, and we say that a point $x$ is acceptable
to the filter $\mathcal{F}$ if and only if

\begin{equation}
f(x) \le f^{(l)} - \gamma h(x) \quad \text{or} \quad h(x) \le \beta h^{(l)}, \quad \forall l \in \mathcal{F}
\label{eq:filter-comparison}
\end{equation}

where $\beta = 0.999$ and $\gamma = 0.001$ are constants.

A nonmonotone filter acceptance criterion is defined by counting the number of filter entries that dominate a new entry
in the sense of \autoref{eq:filter-comparison}. To check whether a point $x$ is acceptable, we define the index set

\begin{equation}
\begin{aligned}
\mathcal{D} = \{ (h^{(l)}, f^{(l)}) \in \mathcal{F} ~|~ & f(x) > f^{(l)} - \gamma h(x) \text{ and } h(x) \ge \beta h^{(l)}, \text{ or} \\
										& f(x) \ge f^{(l)} - \gamma h(x) \text{ and } h(x) > \beta h^{(l)} \}
\end{aligned}
\end{equation}

We say that a point $x$ is acceptable to the nonmonotone filter $\mathcal{F}$ with memory $M > 0$ if and only if the
cardinality $|D| \le M$. For $M = 0$, we recover the standard filter. The nonmonotone filter has also been called a
\textit{shadow filter}.

\todo{add figure}

\subsubsection{Tolerance Tube}

We show how the step computation technique for a tolerance tube would operate. In a tolerance tube,
we accept trust-region steps that lie within a tolerance of infeasibility, defined by an upper bound $U_k>0$.
Initially, this upper bound is set to some user-provided value. At every iteration, we compute the infeasibility
measure as

\[
h(x^{(k)}) := h(c(x^{(k)})) := \max \left( \bigg\lVert \max\left(0, c(x^{(k)}) - \underline{c}\right) \bigg\rVert ,
                                            \bigg\lVert \max\left(0, -c(x^{(k)}) + \overline{c}\right) \bigg\rVert \right) .
\]
We observe, that for a feasible point, $\underline{x} \leq x^* \leq \overline{x}$, we get $h(x^*) = 0$. In a
tube method, we accept a new point $\hat{x}^{(k)}$ if its constraint violation lies below the current upper bounds, i.e.

\[
\hat{x}^{(k)} \; \text{ acceptable } \; \text{if} \; h(\hat{x}^{(k)}) \leq \beta U_k,
\]

where $0 < \beta < 1$ is a constant (typically 0.99). Otherwise, we reject the step, and reduce the trust region.
We adjust the upper bound only on steps where we do not achieve sufficient reduction in the objective
function, that is,

\[
\text{if} \; \Delta q_k < \gamma \min(h_k^2,1) \; \text{ or } \; \Delta f_k < \sigma \Delta q
   \; \text{ then } U_{k+1} := \max \bigg(\beta U_k, h_k -\gamma \max(0, h_k - h_{k+1}) \bigg),
\]
   
where the actual and predicted reduction are computed as usual:

\[
\Delta f_k := f(x^{(k)}) - f(x^{(k+1)}) \; \text{ and } \;
\Delta q_k := -  \nabla f (x^{(k)})^T d^{(k)} - \frac{1}{2} d^{(k)^T} W_1(x^{(k)}, \lambda^{(k)}) d^{(k)},
\]

respectively. Otherwise, we leave the upper bound unchanged, $U_{k+1} := U_k$.

\subsubsection{Augmented Lagrangian}

\subsection{Globalization Mechanisms}

\subsubsection{Trust Region}

\begin{algorithm}[h!]
\caption{Trust-Region Method}
\SetAlgoVlined
Given primal-dual point $(x^{(0)}, \lambda^{(0)}, \nu^{(0)})$ \;
Set $k \gets 0$ \;
\While{termination criteria are not met} {
	Reset trust-region radius $\rho \in [\underline{\rho}, \overline{\rho}]$ \;
	\While{$\hat{x}^{(k)}$ is not acceptable} {
		$(d^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)}) \gets$ solve trust-region problem at $(x^{(k)}, \lambda^{(k)})$ with radius $\rho$ \;
		Compute trial step $s^{(k)} := d^{(k)}$ \;
		Compute trial iterate $\hat{x}^{(k)} := x^{(k)} + s^{(k)}$ \;
		\eIf{$\hat{x}^{(k)}$ is acceptable} {
			\If{trust region active at $d^{(k)}$} {
				Increase radius: $\rho \gets 2 \rho$ \;
			}
		}{
			Decrease radius: $\rho \gets \rho/2$ \;
		}
	}
	Update $(x^{(k+1)}, \lambda^{(k+1)}, \nu^{(k+1)}) \gets (\hat{x}^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)})$ \;
	$k \gets k+1$ \;
}
\end{algorithm}

We note that the choice of constants $2$ and $\frac{1}{2}$ is not crucial. \charlie{keep 2 and 1/2 here, and show
parameterized version in Argonot OR just say increase and decrease}.
\sven{May need to check descend property of step}

\subsubsection{Line Search}

\begin{algorithm}[h!]
\caption{Line-Search Method}
\SetAlgoVlined
Given primal-dual point $(x^{(0)}, \lambda^{(0)}, \nu^{(0)})$ \;
Set $k \gets 0$ \;
\While{termination criteria are not met} {
	$(d^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)}) \gets$ solve subproblem at $(x^{(k)}, \lambda^{(k)})$ \;
	Set $\alpha \gets 1$ \;
	\While{$\hat{x}^{(k)}$ is not acceptable} {
		Compute trial step $s^{(k)} := \alpha d^{(k)}$ \;
		Compute trial iterate $\hat{x}^{(k)} := x^{(k)} + s^{(k)}$ \;
		\If{$\hat{x}^{(k)}$ is not acceptable} {
			Decrease step size: $\alpha \gets \alpha/2$ \;
		}
	}
	Update $(x^{(k+1)}, \lambda^{(k+1)}, \nu^{(k+1)}) \gets (\hat{x}^{(k)}, \hat{\lambda}^{(k)}, \hat{\nu}^{(k)})$ \;
	$k \gets k+1$ \;
}
\end{algorithm}

\clearpage
